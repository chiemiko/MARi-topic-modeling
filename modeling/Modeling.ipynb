{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following file contains: \n",
    "\n",
    "- LDA\n",
    "- Topic Modeling/Coherence\n",
    "- TF-IDF/Bag of Words with Polynomial Model\n",
    "- Similarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import glob\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data_processed/concatenated_data_cleaned_labeled_preprocessed.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA With Gensim\n",
    "\n",
    "list of tokens -> bag-of-words corpus -> dictionary -> bag-of-words corpus -> LDA model\n",
    "\n",
    "\n",
    "\n",
    "from LDA model -> extract top topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_tokens = list(df['desc_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ['scientifically', 'prove', 'method', 'user', ...\n",
       "1       ['overview', 'numbers', 'finding', 'story', 'n...\n",
       "2       ['world', 'premiere', 'travel', 'technology', ...\n",
       "3       ['medical', 'scientist', 'least', 'years', 'cl...\n",
       "4       ['class=\"jobsectionheader\">company', 'overview...\n",
       "5       ['responsible', 'assist', 'development', 'stat...\n",
       "6       ['class=\"jobsectionheader\">overview', 'respons...\n",
       "7       ['description', 'looking', 'highly', 'motivate...\n",
       "8       ['ready', 'challenge', 'celonis', 'leader', 'b...\n",
       "9       ['overview', 'supply', 'chain', 'group', 'asso...\n",
       "10      ['customer', 'operations', 'scientist', 'formu...\n",
       "11      ['report', 'science', 'understand', 'business'...\n",
       "12      ['getty', 'image', 'looking', 'individual', 'e...\n",
       "13      ['class=\"jobsectionheader\">description', 'repo...\n",
       "14      ['jotitle', 'analyst', 'scientist', 'jodescrip...\n",
       "15      ['position', 'works', 'scientist', 'report', '...\n",
       "16      ['scientist', 'intern', 'industry', 'leading',...\n",
       "17      ['summary', 'post', 'weekly', 'hours', 'number...\n",
       "18      ['looking', 'company', 'inspire', 'passion', '...\n",
       "19      ['client', 'amaze', 'company', 'redmond', 'loo...\n",
       "20      ['client', 'seek', 'telecom', 'company', 'bell...\n",
       "21      ['overview', 'centrica', 'world', 'leading', '...\n",
       "22      ['looking', 'scientist', 'develop', 'digital',...\n",
       "23      ['duration', 'month', 'assist', 'development',...\n",
       "24      ['brewing', 'decision', 'scientist', 'visualiz...\n",
       "25      ['focus', 'understanding', 'predict', 'incenti...\n",
       "26      ['basic', 'qualification', 'advance', 'degree'...\n",
       "27      ['brewing', 'scientist', 'tobeapartner', 'begi...\n",
       "28      ['expedia', 'design', 'build', 'machine', 'lea...\n",
       "29      ['position', 'global', 'payment', 'fraud', 'bu...\n",
       "                              ...                        \n",
       "8820    ['pryon', 'serious', 'augment', 'intelligence'...\n",
       "8821    ['interest', 'pursue', 'career', 'asset', 'man...\n",
       "8822    ['twitter', 'window', 'happening', 'world', 'c...\n",
       "8823    ['crave', 'collaborative', 'organization', 'co...\n",
       "8824    ['crave', 'collaborative', 'organization', 'co...\n",
       "8825    ['engineer', 'whoâ\\x80\\x99s', 'interest', 'tac...\n",
       "8826    ['explore', 'possibility', 'across', 'global',...\n",
       "8827    ['officially', 'consider', 'position', 'shanno...\n",
       "8828    ['summary', 'post', 'weekly', 'hours', 'number...\n",
       "8829    ['bachelorâ\\x80\\x99s', 'degree', 'computer', '...\n",
       "8830    ['little', 'decade', 'pitchbook', 'grow', 'glo...\n",
       "8831    ['passionate', 'field', 'science', 'believe', ...\n",
       "8832    ['class=\"jobsectionheader\">about', 'industry',...\n",
       "8833    ['summary', 'post', 'weekly', 'hours', 'number...\n",
       "8834    ['tower', 'crescent', '12066', 'unite', 'state...\n",
       "8835    ['challenge', 'excite', 'prospect', 'unlock', ...\n",
       "8836    ['seeking', 'experience', 'machine', 'learning...\n",
       "8837    ['artificial', 'intelligence', 'machine', 'lea...\n",
       "8838    ['youâ\\x80\\x99ll', 'verizon', 'standing', 'foc...\n",
       "8839    ['-----------', 'years', 'mission', 'increase'...\n",
       "8840    ['client', 'chicago', 'looking', 'machine', 'l...\n",
       "8841    ['position', 'overview', 'stats', 'world', 'cl...\n",
       "8842    ['pubwise', 'building', 'drive', 'technology',...\n",
       "8843    ['looking', 'experience', 'machine', 'learning...\n",
       "8844    ['experience', 'workday', 'company', 'people',...\n",
       "8845    ['experience', 'workday', 'company', 'people',...\n",
       "8846    ['advance', 'machine', 'learning', 'span', 'mo...\n",
       "8847    ['currently', 'seeking', 'engineer', 'indopaco...\n",
       "8848    ['position', 'overview', 'stats', 'world', 'cl...\n",
       "8849    ['summary', 'post', 'weekly', 'hours', 'number...\n",
       "Name: desc_tokens, Length: 8850, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['desc_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-662bfa4b15ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a dictionary from the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create a bag-of-words corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# also called corpus!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;31m# update Dictionary with the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \"\"\"\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "# Create a dictionary from the data\n",
    "dictionary = corpora.Dictionary(desc_tokens)\n",
    "\n",
    "# Create a bag-of-words corpus \n",
    "# also called corpus! \n",
    "doc_term_matrix = [dictionary.doc2bow(text) for text in desc_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "#dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation\n",
    "\n",
    "https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling over all descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 10\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "#ldamodel.save('model5.gensim')\n",
    "\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Optimum Number of Topics to Use\n",
    "### And coherence measurements\n",
    "\n",
    "As seen on\n",
    "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, doc_term_matrix, desc_tokens, stop, start=2, step=3):\n",
    "    \"\"\"\n",
    "    From: https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
    "    Input   : dictionary : Gensim dictionary\n",
    "              corpus : Gensim corpus\n",
    "              texts : List of input texts\n",
    "              stop : Max num of topics\n",
    "    purpose : Compute c_v coherence for various number of topics\n",
    "    Output  : model_list : List of LSA topic models\n",
    "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, stop, step):\n",
    "        # generate LSi model on range of num_topics \n",
    "        model = LsiModel(doc_term_matrix, num_topics=num_topics, id2word = dictionary)  # train model\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=desc_tokens, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(desc_tokens,start, stop, step, dictionary, doc_term_matrix):\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,desc_tokens,\n",
    "                                                            stop, start, step)\n",
    "    # Show graph\n",
    "    x = range(start, stop, step)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.title('Coherence Score to Assess Number of Topics')\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start,stop,step=2,12,1\n",
    "plot_graph(desc_tokens,start,stop,step, dictionary, doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results: \n",
    "Optimal number of topics to evaluate is 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 3\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "#ldamodel.save('model5.gensim')\n",
    "\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "# Create new model using different data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "#lda = gensim.models.ldamodel.LdaModel.load('model5.gensim')\n",
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Model\n",
    "### Using Multinomial Naive Bayes Classification from scikit-learn\n",
    "\n",
    "As seen on: \n",
    "https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk\n",
    "\n",
    "The following model uses the one-hot-encoded description column in the data frame as the set of X features and evaluates the searched role as the response y variable. \n",
    "\n",
    "## 1. Using TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# generate matrix of word vectors \n",
    "tfidf_matrix = vectorizer.fit_transform(descriptions)\n",
    "\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=TfidfVectorizer()\n",
    "text_tf= tf.fit_transform(df['description'])\n",
    "\n",
    "# Build the training and sets using the one-hot-encoded TF-IDF vectorizer\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_tf, df['search_role'], test_size=0.3, random_state=1)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Bag of Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can generate document term matrix by using scikit-learn's CountVectorizer.\n",
    "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts= cv.fit_transform(df['description'])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_counts, df['search_role'], test_size=0.3, random_state=1)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trigram model\n",
    "\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (3,3),tokenizer = token.tokenize)\n",
    "text_counts= cv.fit_transform(df['description'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_counts, df['search_role'], test_size=0.3, random_state=1)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Similarity Between Aggregated Posts Grouped by Search Role\n",
    "- Uses Cosine Similarity score\n",
    "\n",
    "As seen on:\n",
    "https://www.datacamp.com/courses/feature-engineering-for-nlp-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and create Doc object\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "for i, merged_text in enumerate(descriptions):\n",
    "    doc = nlp(merged_text)\n",
    "\n",
    "\n",
    "    for token1 in doc:\n",
    "        for token2 in doc:\n",
    "            print(token1.text, token2.text, token1.similarity(token2))\n",
    "            print()\n",
    "        #print(token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
