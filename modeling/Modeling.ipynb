{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following file contains: \n",
    "\n",
    "- LDA\n",
    "- Topic Modeling/Coherence\n",
    "- TF-IDF/Bag of Words with Polynomial Model\n",
    "- Similarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import glob\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data_processed/concatenated_data_cleaned_labeled_preprocessed_alt.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA With Gensim\n",
    "As seen on: https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\n",
    "\n",
    "list of tokens -> bag-of-words corpus -> dictionary -> bag-of-words corpus -> LDA model\n",
    "\n",
    "\n",
    "\n",
    "from LDA model -> extract top topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation of Gensim\n",
    "\n",
    "https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['noom',\n",
       "  'scientifically',\n",
       "  'proven',\n",
       "  'method',\n",
       "  'help',\n",
       "  'user',\n",
       "  'create',\n",
       "  'healthier',\n",
       "  'lifestyle',\n",
       "  'manage',\n",
       "  'important',\n",
       "  'condition',\n",
       "  'like',\n",
       "  'ii',\n",
       "  'diabetes',\n",
       "  'obesity',\n",
       "  'hypertension',\n",
       "  'engineering',\n",
       "  'team',\n",
       "  'forefront',\n",
       "  'challenge',\n",
       "  'solving',\n",
       "  'complex',\n",
       "  'technical',\n",
       "  'problem',\n",
       "  'center',\n",
       "  'around',\n",
       "  'habit',\n",
       "  'behavior',\n",
       "  'lifestyle',\n",
       "  'looking',\n",
       "  'data',\n",
       "  'engineer',\n",
       "  'join',\n",
       "  'data',\n",
       "  'team',\n",
       "  'help',\n",
       "  'u',\n",
       "  'improve',\n",
       "  'maintain',\n",
       "  'data',\n",
       "  'warehouse',\n",
       "  'like',\n",
       "  'billion',\n",
       "  'row',\n",
       "  'data',\n",
       "  'center',\n",
       "  'data',\n",
       "  'driven',\n",
       "  'decision',\n",
       "  'love',\n",
       "  'like',\n",
       "  'u',\n",
       "  'problem',\n",
       "  'affect',\n",
       "  'life',\n",
       "  'real',\n",
       "  'people',\n",
       "  'user',\n",
       "  'depend',\n",
       "  'u',\n",
       "  'make',\n",
       "  'positive',\n",
       "  'change',\n",
       "  'health',\n",
       "  'life',\n",
       "  'base',\n",
       "  'scientifically',\n",
       "  'proven',\n",
       "  'peer',\n",
       "  'reviewed',\n",
       "  'methodology',\n",
       "  'designed',\n",
       "  'medical',\n",
       "  'professionalswe',\n",
       "  'respectful',\n",
       "  'diverse',\n",
       "  'dynamic',\n",
       "  'environment',\n",
       "  'engineering',\n",
       "  'first',\n",
       "  'variety',\n",
       "  'interesting',\n",
       "  'problem',\n",
       "  'affect',\n",
       "  'life',\n",
       "  'real',\n",
       "  'people',\n",
       "  'offer',\n",
       "  'budget',\n",
       "  'personal',\n",
       "  'development',\n",
       "  'expense',\n",
       "  'like',\n",
       "  'training',\n",
       "  'course',\n",
       "  'conference',\n",
       "  'book',\n",
       "  'get',\n",
       "  'three',\n",
       "  'week',\n",
       "  'paid',\n",
       "  'flexible',\n",
       "  'policy',\n",
       "  'remote',\n",
       "  'family',\n",
       "  'friendly',\n",
       "  '50',\n",
       "  'engineering',\n",
       "  'team',\n",
       "  'fully',\n",
       "  'remote',\n",
       "  'worry',\n",
       "  'result',\n",
       "  'spent',\n",
       "  'seat',\n",
       "  'delicious',\n",
       "  'nutritious',\n",
       "  'daily',\n",
       "  'lunch',\n",
       "  'snack',\n",
       "  'prepared',\n",
       "  'sam',\n",
       "  'nyc',\n",
       "  'office',\n",
       "  'site',\n",
       "  'chef',\n",
       "  'like',\n",
       "  'dealing',\n",
       "  'data',\n",
       "  'scale',\n",
       "  'processing',\n",
       "  'transforming',\n",
       "  'hundred',\n",
       "  'million',\n",
       "  'data',\n",
       "  'point',\n",
       "  'per',\n",
       "  'day',\n",
       "  'first',\n",
       "  'rate',\n",
       "  'sql',\n",
       "  'aware',\n",
       "  'limit',\n",
       "  'know',\n",
       "  'better',\n",
       "  'find',\n",
       "  'different',\n",
       "  'solution',\n",
       "  'familiar',\n",
       "  'etl',\n",
       "  'tool',\n",
       "  'problem',\n",
       "  'airflow',\n",
       "  'redshift',\n",
       "  'glue',\n",
       "  'many',\n",
       "  'system',\n",
       "  'used',\n",
       "  'alongside',\n",
       "  'data',\n",
       "  'analyst',\n",
       "  'data',\n",
       "  'scientist',\n",
       "  'help',\n",
       "  'prepare',\n",
       "  'complex',\n",
       "  'datasets',\n",
       "  'used',\n",
       "  'solve',\n",
       "  'difficult',\n",
       "  'problem'],\n",
       " ['love',\n",
       "  'number',\n",
       "  'finding',\n",
       "  'story',\n",
       "  'number',\n",
       "  'thought',\n",
       "  'tackling',\n",
       "  'complex',\n",
       "  'data',\n",
       "  'issue',\n",
       "  'make',\n",
       "  'smile',\n",
       "  'got',\n",
       "  'knack',\n",
       "  'solving',\n",
       "  'problem',\n",
       "  'want',\n",
       "  'help',\n",
       "  'drive',\n",
       "  'result',\n",
       "  'multi',\n",
       "  'million',\n",
       "  'dollar',\n",
       "  'business',\n",
       "  'answered',\n",
       "  'yes',\n",
       "  'question',\n",
       "  'lead',\n",
       "  'data',\n",
       "  'scientist',\n",
       "  'position',\n",
       "  'strategic',\n",
       "  'financial',\n",
       "  'solution',\n",
       "  'may',\n",
       "  'right',\n",
       "  'fit',\n",
       "  'strategic',\n",
       "  'looking',\n",
       "  'experienced',\n",
       "  'data',\n",
       "  'science',\n",
       "  'leader',\n",
       "  'extensive',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'join',\n",
       "  'data',\n",
       "  'science',\n",
       "  'team',\n",
       "  'already',\n",
       "  'produce',\n",
       "  'cutting',\n",
       "  'edge',\n",
       "  'model',\n",
       "  'predictive',\n",
       "  'prescriptive',\n",
       "  'analytics',\n",
       "  'person',\n",
       "  'would',\n",
       "  'responsible',\n",
       "  'leading',\n",
       "  'focused',\n",
       "  'team',\n",
       "  'conducting',\n",
       "  'data',\n",
       "  'analysis',\n",
       "  'developing',\n",
       "  'predictive',\n",
       "  'model',\n",
       "  'leveraging',\n",
       "  'data',\n",
       "  'science',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'solve',\n",
       "  'various',\n",
       "  'business',\n",
       "  'case',\n",
       "  'including',\n",
       "  'marketing',\n",
       "  'intelligence',\n",
       "  'customer',\n",
       "  'segmentation',\n",
       "  'predictive',\n",
       "  'model',\n",
       "  'operation',\n",
       "  'looking',\n",
       "  'data',\n",
       "  'scientist',\n",
       "  'support',\n",
       "  'product',\n",
       "  'sale',\n",
       "  'leadership',\n",
       "  'marketing',\n",
       "  'team',\n",
       "  'insight',\n",
       "  'gained',\n",
       "  'analyzing',\n",
       "  'external',\n",
       "  'data',\n",
       "  'ideal',\n",
       "  'candidate',\n",
       "  'adept',\n",
       "  'using',\n",
       "  'large',\n",
       "  'data',\n",
       "  'set',\n",
       "  'find',\n",
       "  'opportunity',\n",
       "  'product',\n",
       "  'process',\n",
       "  'optimization',\n",
       "  'using',\n",
       "  'model',\n",
       "  'test',\n",
       "  'effectiveness',\n",
       "  'different',\n",
       "  'course',\n",
       "  'action',\n",
       "  'must',\n",
       "  'strong',\n",
       "  'using',\n",
       "  'variety',\n",
       "  'data',\n",
       "  'mining',\n",
       "  'data',\n",
       "  'analysis',\n",
       "  'method',\n",
       "  'using',\n",
       "  'variety',\n",
       "  'data',\n",
       "  'tool',\n",
       "  'building',\n",
       "  'implementing',\n",
       "  'model',\n",
       "  'using',\n",
       "  'creating',\n",
       "  'algorithm',\n",
       "  'creating',\n",
       "  'running',\n",
       "  'simulation',\n",
       "  'must',\n",
       "  'proven',\n",
       "  'drive',\n",
       "  'business',\n",
       "  'result',\n",
       "  'data',\n",
       "  'based',\n",
       "  'insight',\n",
       "  'must',\n",
       "  'comfortable',\n",
       "  'wide',\n",
       "  'range',\n",
       "  'stakeholder',\n",
       "  'functional',\n",
       "  'team',\n",
       "  'right',\n",
       "  'candidate',\n",
       "  'passion',\n",
       "  'discovering',\n",
       "  'solution',\n",
       "  'hidden',\n",
       "  'large',\n",
       "  'data',\n",
       "  'set',\n",
       "  'stakeholder',\n",
       "  'improve',\n",
       "  'business',\n",
       "  'outcome',\n",
       "  'stakeholder',\n",
       "  'identify',\n",
       "  'opportunity',\n",
       "  'leveraging',\n",
       "  'data',\n",
       "  'drive',\n",
       "  'business',\n",
       "  'analytics',\n",
       "  'solution',\n",
       "  'research',\n",
       "  'develop',\n",
       "  'statistical',\n",
       "  'learning',\n",
       "  'model',\n",
       "  'data',\n",
       "  'analysis',\n",
       "  'communicate',\n",
       "  'result',\n",
       "  'idea',\n",
       "  'key',\n",
       "  'decision',\n",
       "  'maker',\n",
       "  'identify',\n",
       "  'valuable',\n",
       "  'data',\n",
       "  'source',\n",
       "  'automate',\n",
       "  'collection',\n",
       "  'process',\n",
       "  'undertake',\n",
       "  'preprocessing',\n",
       "  'structured',\n",
       "  'unstructured',\n",
       "  'data',\n",
       "  'analyze',\n",
       "  'large',\n",
       "  'amount',\n",
       "  'information',\n",
       "  'discover',\n",
       "  'trend',\n",
       "  'pattern',\n",
       "  'build',\n",
       "  'predictive',\n",
       "  'model',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'algorithm',\n",
       "  'present',\n",
       "  'information',\n",
       "  'using',\n",
       "  'data',\n",
       "  'visualization',\n",
       "  'technique',\n",
       "  'collaborate',\n",
       "  'sale',\n",
       "  'marketing',\n",
       "  'senior',\n",
       "  'executive',\n",
       "  'team',\n",
       "  'qualification',\n",
       "  'relevant',\n",
       "  'graduate',\n",
       "  'degree',\n",
       "  'statistic',\n",
       "  'data',\n",
       "  'science',\n",
       "  'applied',\n",
       "  'math',\n",
       "  'operation',\n",
       "  'research',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'strong',\n",
       "  'problem',\n",
       "  'solving',\n",
       "  'emphasis',\n",
       "  'sale',\n",
       "  'marketing',\n",
       "  'predictive',\n",
       "  'analytics',\n",
       "  'using',\n",
       "  'statistical',\n",
       "  'computer',\n",
       "  'language',\n",
       "  'r',\n",
       "  'python',\n",
       "  'sql',\n",
       "  'etc',\n",
       "  'manipulate',\n",
       "  'data',\n",
       "  'draw',\n",
       "  'insight',\n",
       "  'large',\n",
       "  'data',\n",
       "  'set',\n",
       "  'knowledge',\n",
       "  'variety',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'technique',\n",
       "  'clustering',\n",
       "  'decision',\n",
       "  'tree',\n",
       "  'learning',\n",
       "  'artificial',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'etc',\n",
       "  'real',\n",
       "  'world',\n",
       "  'advantage',\n",
       "  'drawback',\n",
       "  'knowledge',\n",
       "  'statistical',\n",
       "  'data',\n",
       "  'mining',\n",
       "  'technique',\n",
       "  'glm',\n",
       "  'regression',\n",
       "  'random',\n",
       "  'forest',\n",
       "  'boosting',\n",
       "  'tree',\n",
       "  'text',\n",
       "  'mining',\n",
       "  'social',\n",
       "  'network',\n",
       "  'analysis',\n",
       "  'etc',\n",
       "  'knowledge',\n",
       "  'advanced',\n",
       "  'statistical',\n",
       "  'technique',\n",
       "  'concept',\n",
       "  'regression',\n",
       "  'property',\n",
       "  'distribution',\n",
       "  'statistical',\n",
       "  'test',\n",
       "  'proper',\n",
       "  'usage',\n",
       "  'etc',\n",
       "  'application',\n",
       "  'excellent',\n",
       "  'written',\n",
       "  'verbal',\n",
       "  'communication',\n",
       "  'coordinating',\n",
       "  'across',\n",
       "  'team',\n",
       "  'drive',\n",
       "  'learn',\n",
       "  'master',\n",
       "  'new',\n",
       "  'technology',\n",
       "  'technique',\n",
       "  'using',\n",
       "  'business',\n",
       "  'intelligence',\n",
       "  'tool',\n",
       "  'e',\n",
       "  'g',\n",
       "  'powerbi',\n",
       "  'tableau',\n",
       "  'data',\n",
       "  'framework',\n",
       "  'e',\n",
       "  'g',\n",
       "  'hadoop',\n",
       "  'strategic',\n",
       "  'strategic',\n",
       "  'financial',\n",
       "  'solution',\n",
       "  'leading',\n",
       "  'consumer',\n",
       "  'finance',\n",
       "  'specializes',\n",
       "  'helping',\n",
       "  'people',\n",
       "  'much',\n",
       "  'credit',\n",
       "  'card',\n",
       "  'debt',\n",
       "  'recently',\n",
       "  'named',\n",
       "  '21st',\n",
       "  'best',\n",
       "  'new',\n",
       "  'york',\n",
       "  'best',\n",
       "  'company',\n",
       "  'certified',\n",
       "  'great',\n",
       "  'place',\n",
       "  'time',\n",
       "  'additional',\n",
       "  'honor',\n",
       "  'include',\n",
       "  'named',\n",
       "  'two',\n",
       "  'time',\n",
       "  '50',\n",
       "  'fastest',\n",
       "  'growing',\n",
       "  'company',\n",
       "  'new',\n",
       "  'york',\n",
       "  'city',\n",
       "  'prestigious',\n",
       "  'inc',\n",
       "  '500',\n",
       "  'list',\n",
       "  '500',\n",
       "  'fastest',\n",
       "  'growing',\n",
       "  'company']]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Fixing desc_tokens column'''\n",
    "#df_test = df.iloc[0:10, :]\n",
    "\n",
    "\n",
    "desc_tokens = list(df['new_description'])\n",
    "\n",
    "new_col = []\n",
    "for i, item in enumerate(desc_tokens):\n",
    "    listed = list(item.split())\n",
    "    new_col.append(listed)\n",
    "\n",
    "df['desc_tokens'] = new_col\n",
    "\n",
    "desc_tokens = list(df['desc_tokens'])\n",
    "desc_tokens[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dictionary from aggregated data descriptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from the data\n",
    "dictionary = corpora.Dictionary(desc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8880\n",
      "[[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 2), (14, 1), (15, 1), (16, 1), (17, 2), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 9), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 3), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 2), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 3), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 3), (66, 2), (67, 5), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 2), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 5), (97, 1), (98, 1), (99, 2), (100, 1), (101, 2), (102, 1), (103, 2), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 2), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 3), (122, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 3), (128, 2), (129, 2), (130, 1), (131, 1), (132, 1), (133, 1)], [(0, 1), (17, 1), (20, 1), (23, 24), (27, 2), (33, 1), (45, 1), (56, 1), (61, 1), (63, 1), (69, 2), (70, 1), (73, 1), (77, 1), (79, 1), (88, 1), (96, 2), (99, 1), (101, 1), (105, 3), (111, 2), (115, 4), (116, 1), (117, 2), (119, 1), (121, 6), (124, 2), (130, 3), (134, 1), (135, 2), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 2), (143, 1), (144, 1), (145, 4), (146, 3), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 2), (156, 1), (157, 1), (158, 1), (159, 6), (160, 2), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 3), (172, 2), (173, 1), (174, 1), (175, 1), (176, 1), (177, 2), (178, 1), (179, 1), (180, 1), (181, 1), (182, 1), (183, 1), (184, 1), (185, 1), (186, 1), (187, 1), (188, 1), (189, 1), (190, 1), (191, 4), (192, 2), (193, 1), (194, 1), (195, 1), (196, 4), (197, 1), (198, 1), (199, 1), (200, 1), (201, 1), (202, 2), (203, 1), (204, 2), (205, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (211, 2), (212, 1), (213, 1), (214, 1), (215, 1), (216, 1), (217, 2), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 2), (225, 1), (226, 1), (227, 1), (228, 1), (229, 2), (230, 3), (231, 2), (232, 1), (233, 1), (234, 1), (235, 3), (236, 1), (237, 4), (238, 1), (239, 1), (240, 1), (241, 2), (242, 1), (243, 6), (244, 2), (245, 1), (246, 4), (247, 1), (248, 1), (249, 4), (250, 1), (251, 1), (252, 1), (253, 3), (254, 7), (255, 1), (256, 1), (257, 3), (258, 2), (259, 2), (260, 1), (261, 3), (262, 2), (263, 2), (264, 2), (265, 1), (266, 1), (267, 1), (268, 1), (269, 1), (270, 1), (271, 1), (272, 1), (273, 5), (274, 1), (275, 1), (276, 1), (277, 1), (278, 2), (279, 1), (280, 2), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (286, 1), (287, 1), (288, 1), (289, 1), (290, 2), (291, 1), (292, 2), (293, 1), (294, 2), (295, 1), (296, 3), (297, 5), (298, 1), (299, 1), (300, 3), (301, 1), (302, 1), (303, 1), (304, 1), (305, 1), (306, 3), (307, 1), (308, 5), (309, 1), (310, 4), (311, 2), (312, 1), (313, 1), (314, 1), (315, 1), (316, 5), (317, 1), (318, 2), (319, 1), (320, 1), (321, 2), (322, 2), (323, 1), (324, 1), (325, 1), (326, 1), (327, 1), (328, 8), (329, 1), (330, 1), (331, 1), (332, 1), (333, 1), (334, 1), (335, 1), (336, 1), (337, 1), (338, 1), (339, 2)]]\n"
     ]
    }
   ],
   "source": [
    "# Create a bag-of-words corpus \n",
    "# also called corpus! \n",
    "\n",
    "doc_term_matrix = [dictionary.doc2bow(text) for text in desc_tokens]\n",
    "print(len(doc_term_matrix))\n",
    "print(doc_term_matrix[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(doc_term_matrix, open('doc_term_matrix.pkl', 'wb'))\n",
    "#dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of Descriptions Grouped by Bin Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Analyst\n",
      "Data Analyst\n",
      "Data Engineer\n",
      "Data Scientist\n",
      "ML/AI Researcher\n",
      "Machine Learning Engineer\n",
      "Software Engineer\n",
      "Statistical Modeler/Researcher\n",
      "Unclassified\n"
     ]
    }
   ],
   "source": [
    "by_role = df.groupby('bin_role')\n",
    "\n",
    "'''Creates a dictionary called by_role_tokens where an aggregated list of tokens is saved for each bin role\n",
    "NOTE: that desc_tokens should be a list of lists - one list of words per document'''\n",
    "\n",
    "by_role_tokens = {}\n",
    "\n",
    "for key, item in by_role:\n",
    "    aggregated_tokens = []\n",
    "    print(key)\n",
    "    #print(item['desc_tokens'])\n",
    "    desc_tokens = list(item['desc_tokens'])\n",
    "\n",
    "    #for item2 in item['desc_tokens']:\n",
    "        #aggregated_tokens = aggregated_tokens + item2\n",
    "    by_role_tokens[key] = desc_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling for each bin_role group\n",
    "\n",
    "### Question --> Can we use the same dictionary for each subject/bin role subject analysis? Or should we create new models for each role? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Analyst\n",
      "(0, '0.026*\"business\" + 0.012*\"project\" + 0.011*\"process\" + 0.010*\"system\"')\n",
      "(1, '0.028*\"business\" + 0.023*\"data\" + 0.011*\"team\" + 0.007*\"reporting\"')\n",
      "(2, '0.018*\"data\" + 0.011*\"business\" + 0.009*\"management\" + 0.009*\"system\"')\n",
      "\n",
      "Data Analyst\n",
      "(0, '0.037*\"data\" + 0.012*\"business\" + 0.008*\"team\" + 0.008*\"analysis\"')\n",
      "(1, '0.022*\"data\" + 0.006*\"analysis\" + 0.006*\"position\" + 0.006*\"preferred\"')\n",
      "(2, '0.031*\"data\" + 0.006*\"analytics\" + 0.006*\"team\" + 0.006*\"process\"')\n",
      "\n",
      "Data Engineer\n",
      "(0, '0.036*\"u\" + 0.016*\"data\" + 0.010*\"learning\" + 0.010*\"team\"')\n",
      "(1, '0.033*\"data\" + 0.008*\"cloud\" + 0.008*\"technology\" + 0.008*\"team\"')\n",
      "(2, '0.051*\"data\" + 0.009*\"business\" + 0.008*\"team\" + 0.006*\"design\"')\n",
      "\n",
      "Data Scientist\n",
      "(0, '0.025*\"data\" + 0.007*\"team\" + 0.007*\"business\" + 0.006*\"model\"')\n",
      "(1, '0.038*\"data\" + 0.010*\"science\" + 0.009*\"learning\" + 0.009*\"business\"')\n",
      "(2, '0.029*\"data\" + 0.014*\"business\" + 0.009*\"science\" + 0.008*\"learning\"')\n",
      "\n",
      "ML/AI Researcher\n",
      "(0, '0.028*\"learning\" + 0.024*\"machine\" + 0.017*\"data\" + 0.013*\"team\"')\n",
      "(1, '0.020*\"learning\" + 0.014*\"machine\" + 0.011*\"data\" + 0.011*\"system\"')\n",
      "(2, '0.013*\"data\" + 0.008*\"business\" + 0.008*\"ml\" + 0.006*\"solution\"')\n",
      "\n",
      "Machine Learning Engineer\n",
      "(0, '0.015*\"data\" + 0.007*\"machine\" + 0.007*\"learning\" + 0.006*\"interview\"')\n",
      "(1, '0.009*\"computer\" + 0.007*\"team\" + 0.006*\"technology\" + 0.006*\"innovation\"')\n",
      "(2, '0.012*\"data\" + 0.007*\"learning\" + 0.006*\"team\" + 0.006*\"technology\"')\n",
      "\n",
      "Software Engineer\n",
      "(0, '0.011*\"team\" + 0.007*\"software\" + 0.007*\"learning\" + 0.007*\"engineer\"')\n",
      "(1, '0.017*\"software\" + 0.011*\"development\" + 0.010*\"team\" + 0.008*\"system\"')\n",
      "(2, '0.011*\"team\" + 0.010*\"software\" + 0.009*\"product\" + 0.007*\"technology\"')\n",
      "\n",
      "Statistical Modeler/Researcher\n",
      "(0, '0.011*\"model\" + 0.009*\"business\" + 0.009*\"capital\" + 0.006*\"modeling\"')\n",
      "(1, '0.005*\"model\" + 0.004*\"data\" + 0.003*\"modeling\" + 0.003*\"relevant\"')\n",
      "(2, '0.016*\"data\" + 0.008*\"model\" + 0.008*\"statistical\" + 0.007*\"risk\"')\n",
      "\n",
      "Unclassified\n",
      "(0, '0.019*\"research\" + 0.007*\"position\" + 0.006*\"program\" + 0.006*\"data\"')\n",
      "(1, '0.013*\"team\" + 0.010*\"data\" + 0.008*\"product\" + 0.007*\"ã\"')\n",
      "(2, '0.009*\"management\" + 0.008*\"business\" + 0.008*\"risk\" + 0.007*\"project\"')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 3 subjects/topics for each bin_role grouping\n",
    "\n",
    "NUM_TOPICS = 3\n",
    "\n",
    "for key, values in by_role_tokens.items():\n",
    "    \n",
    "    doc_term_matrix = [dictionary.doc2bow(text) for text in values]\n",
    "    \n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "\n",
    "    topics = ldamodel.print_topics(num_words=4)\n",
    "    print(key)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same as above but with new dictionaries for each bin role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Analyst\n",
      "(0, '0.025*\"data\" + 0.019*\"business\" + 0.010*\"team\" + 0.006*\"system\"')\n",
      "(1, '0.033*\"business\" + 0.016*\"data\" + 0.011*\"process\" + 0.011*\"project\"')\n",
      "(2, '0.019*\"business\" + 0.008*\"analysis\" + 0.008*\"project\" + 0.007*\"system\"')\n",
      "\n",
      "Data Analyst\n",
      "(0, '0.034*\"data\" + 0.013*\"business\" + 0.009*\"analysis\" + 0.008*\"team\"')\n",
      "(1, '0.043*\"data\" + 0.013*\"business\" + 0.009*\"team\" + 0.007*\"analytics\"')\n",
      "(2, '0.031*\"data\" + 0.007*\"information\" + 0.006*\"management\" + 0.006*\"support\"')\n",
      "\n",
      "Data Engineer\n",
      "(0, '0.044*\"u\" + 0.021*\"data\" + 0.014*\"learning\" + 0.011*\"team\"')\n",
      "(1, '0.047*\"data\" + 0.008*\"team\" + 0.008*\"business\" + 0.008*\"system\"')\n",
      "(2, '0.043*\"data\" + 0.009*\"technology\" + 0.009*\"team\" + 0.009*\"cloud\"')\n",
      "\n",
      "Data Scientist\n",
      "(0, '0.037*\"data\" + 0.011*\"learning\" + 0.010*\"science\" + 0.009*\"team\"')\n",
      "(1, '0.037*\"data\" + 0.012*\"business\" + 0.009*\"science\" + 0.008*\"team\"')\n",
      "(2, '0.027*\"data\" + 0.009*\"business\" + 0.008*\"team\" + 0.007*\"learning\"')\n",
      "\n",
      "ML/AI Researcher\n",
      "(0, '0.034*\"learning\" + 0.027*\"machine\" + 0.012*\"team\" + 0.009*\"deep\"')\n",
      "(1, '0.018*\"data\" + 0.016*\"learning\" + 0.013*\"machine\" + 0.008*\"solution\"')\n",
      "(2, '0.021*\"data\" + 0.015*\"learning\" + 0.014*\"team\" + 0.012*\"machine\"')\n",
      "\n",
      "Machine Learning Engineer\n",
      "(0, '0.022*\"data\" + 0.017*\"learning\" + 0.013*\"engineering\" + 0.013*\"computer\"')\n",
      "(1, '0.031*\"data\" + 0.013*\"technology\" + 0.009*\"develop\" + 0.009*\"team\"')\n",
      "(2, '0.012*\"computer\" + 0.012*\"data\" + 0.011*\"team\" + 0.011*\"learning\"')\n",
      "\n",
      "Software Engineer\n",
      "(0, '0.018*\"software\" + 0.011*\"development\" + 0.010*\"team\" + 0.010*\"system\"')\n",
      "(1, '0.012*\"team\" + 0.012*\"software\" + 0.008*\"development\" + 0.007*\"technology\"')\n",
      "(2, '0.014*\"team\" + 0.014*\"software\" + 0.009*\"system\" + 0.009*\"data\"')\n",
      "\n",
      "Statistical Modeler/Researcher\n",
      "(0, '0.039*\"data\" + 0.014*\"science\" + 0.012*\"training\" + 0.010*\"analytics\"')\n",
      "(1, '0.019*\"model\" + 0.018*\"data\" + 0.013*\"risk\" + 0.011*\"modeling\"')\n",
      "(2, '0.021*\"business\" + 0.021*\"model\" + 0.014*\"statistical\" + 0.013*\"data\"')\n",
      "\n",
      "Unclassified\n",
      "(0, '0.008*\"management\" + 0.007*\"project\" + 0.007*\"risk\" + 0.007*\"business\"')\n",
      "(1, '0.013*\"team\" + 0.008*\"product\" + 0.008*\"data\" + 0.008*\"technology\"')\n",
      "(2, '0.018*\"research\" + 0.012*\"data\" + 0.010*\"ã\" + 0.006*\"analysis\"')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 3 subjects/topics for each bin_role grouping\n",
    "\n",
    "NUM_TOPICS = 3\n",
    "\n",
    "for key, values in by_role_tokens.items():\n",
    "    \n",
    "    # Create a dictionary from the data\n",
    "    dictionary = corpora.Dictionary(values)\n",
    "    \n",
    "    doc_term_matrix = [dictionary.doc2bow(text) for text in values]\n",
    "    \n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "\n",
    "    topics = ldamodel.print_topics(num_words=4)\n",
    "    print(key)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyLDAvis\n",
    "\n",
    "Establishes most salient terms and creates distance plot maps of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "#lda = gensim.models.ldamodel.LdaModel.load('model5.gensim')\n",
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Model\n",
    "### Using Multinomial Naive Bayes Classification from scikit-learn\n",
    "\n",
    "As seen on: \n",
    "https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk\n",
    "\n",
    "The following model uses the one-hot-encoded description column in the data frame as the set of X features and evaluates the searched role as the response y variable. \n",
    "\n",
    "## 1. Using TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_descriptions(by_role):\n",
    "    '''Creates concatenated/joined text for job descriptions for each bin role'''\n",
    "    roles = []\n",
    "    merged_desc = []\n",
    "    for key, items in by_role.indices.items():\n",
    "        # concatenate strings in descriptions column (by search_role grouping) and append to list of merged descriptions\n",
    "        string=(\" \").join(description for description in df.loc[items,'new_description'])\n",
    "        merged_desc.append(string)\n",
    "        roles.append(key)\n",
    "\n",
    "    return merged_desc, roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''descriptions variable is list of concatenated descriptions separated by bin role'''\n",
    "descriptions, roles = group_descriptions(by_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6511)\t0.001014438350062388\n",
      "  (0, 19322)\t0.020021007121598784\n",
      "  (0, 749)\t0.0033523750826131636\n",
      "  (0, 1)\t0.025074853579478088\n",
      "  (0, 807)\t0.008677986892557077\n",
      "  (0, 24218)\t0.03479378907539983\n",
      "  (0, 7354)\t0.0023493786852747823\n",
      "  (0, 21170)\t0.00558719704041346\n",
      "  (0, 16946)\t0.001783147991621317\n",
      "  (0, 4020)\t0.03372470617084844\n",
      "  (0, 1968)\t0.0008321357294232813\n",
      "  (0, 13028)\t0.005468320507638706\n",
      "  (0, 16754)\t0.005824950105962969\n",
      "  (0, 3666)\t0.0008227794116481934\n",
      "  (0, 13496)\t0.02449387481723123\n",
      "  (0, 18733)\t0.03178091907166409\n",
      "  (0, 13504)\t0.018155547561895077\n",
      "  (0, 5492)\t0.01418964582404574\n",
      "  (0, 9245)\t0.01380088840420887\n",
      "  (0, 15423)\t0.0013606509694290435\n",
      "  (0, 8584)\t0.0005274174507629513\n",
      "  (0, 3024)\t0.002936723356593478\n",
      "  (0, 21900)\t0.08970577462735765\n",
      "  (0, 20617)\t0.004160678647116406\n",
      "  (0, 4711)\t0.07551612880331192\n",
      "  :\t:\n",
      "  (8, 9754)\t0.0004560426821465608\n",
      "  (8, 16299)\t7.600711369109347e-05\n",
      "  (8, 14881)\t7.600711369109347e-05\n",
      "  (8, 13816)\t0.00015201422738218693\n",
      "  (8, 24323)\t7.600711369109347e-05\n",
      "  (8, 14831)\t7.600711369109347e-05\n",
      "  (8, 24380)\t7.600711369109347e-05\n",
      "  (8, 3329)\t7.600711369109347e-05\n",
      "  (8, 24329)\t7.600711369109347e-05\n",
      "  (8, 14635)\t7.600711369109347e-05\n",
      "  (8, 13653)\t7.600711369109347e-05\n",
      "  (8, 14537)\t0.00015201422738218693\n",
      "  (8, 9753)\t7.600711369109347e-05\n",
      "  (8, 2946)\t7.600711369109347e-05\n",
      "  (8, 12513)\t7.600711369109347e-05\n",
      "  (8, 2875)\t7.600711369109347e-05\n",
      "  (8, 10255)\t7.600711369109347e-05\n",
      "  (8, 17814)\t7.600711369109347e-05\n",
      "  (8, 21028)\t7.600711369109347e-05\n",
      "  (8, 13524)\t7.600711369109347e-05\n",
      "  (8, 3347)\t7.600711369109347e-05\n",
      "  (8, 14589)\t7.600711369109347e-05\n",
      "  (8, 3328)\t7.600711369109347e-05\n",
      "  (8, 17754)\t7.600711369109347e-05\n",
      "  (8, 17973)\t7.600711369109347e-05\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# generate matrix of word vectors \n",
    "tfidf_matrix = vectorizer.fit_transform(descriptions)\n",
    "\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.4804804804804805\n"
     ]
    }
   ],
   "source": [
    "tf=TfidfVectorizer()\n",
    "text_tf= tf.fit_transform(df['description'])\n",
    "\n",
    "# Build the training and sets using the one-hot-encoded TF-IDF vectorizer\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_tf, df['bin_role'], test_size=0.3, random_state=1)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Bag of Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can generate document term matrix by using scikit-learn's CountVectorizer.\n",
    "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts= cv.fit_transform(df['description'])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_counts, df['search_role'], test_size=0.3, random_state=1)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trigram model\n",
    "\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (3,3),tokenizer = token.tokenize)\n",
    "text_counts= cv.fit_transform(df['description'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_counts, df['search_role'], test_size=0.3, random_state=1)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Similarity Between Aggregated Posts Grouped by Search Role\n",
    "- Uses Cosine Similarity score\n",
    "\n",
    "As seen on:\n",
    "https://www.datacamp.com/courses/feature-engineering-for-nlp-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and create Doc object\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "for i, merged_text in enumerate(descriptions):\n",
    "    doc = nlp(merged_text)\n",
    "\n",
    "\n",
    "    for token1 in doc:\n",
    "        for token2 in doc:\n",
    "            print(token1.text, token2.text, token1.similarity(token2))\n",
    "            print()\n",
    "        #print(token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
