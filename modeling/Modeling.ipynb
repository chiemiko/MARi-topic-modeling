{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following file contains: \n",
    "\n",
    "- LDA\n",
    "- Topic Modeling/Coherence\n",
    "- TF-IDF/Bag of Words with Polynomial Model\n",
    "- Similarity Scores (Work in progress!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import glob\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data_processed/concatenated_data_cleaned_labeled_preprocessed_alt.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA With Gensim\n",
    "As seen on: https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\n",
    "\n",
    "list of tokens -> bag-of-words corpus -> dictionary -> bag-of-words corpus -> LDA model\n",
    "\n",
    "\n",
    "\n",
    "from LDA model -> extract top topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation of Gensim\n",
    "\n",
    "https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['noom',\n",
       "  'scientifically',\n",
       "  'proven',\n",
       "  'method',\n",
       "  'help',\n",
       "  'user',\n",
       "  'create',\n",
       "  'healthier',\n",
       "  'lifestyle',\n",
       "  'manage',\n",
       "  'important',\n",
       "  'condition',\n",
       "  'like',\n",
       "  'ii',\n",
       "  'diabetes',\n",
       "  'obesity',\n",
       "  'hypertension',\n",
       "  'engineering',\n",
       "  'team',\n",
       "  'forefront',\n",
       "  'challenge',\n",
       "  'solving',\n",
       "  'complex',\n",
       "  'technical',\n",
       "  'problem',\n",
       "  'center',\n",
       "  'around',\n",
       "  'habit',\n",
       "  'behavior',\n",
       "  'lifestyle',\n",
       "  'looking',\n",
       "  'data',\n",
       "  'engineer',\n",
       "  'join',\n",
       "  'data',\n",
       "  'team',\n",
       "  'help',\n",
       "  'u',\n",
       "  'improve',\n",
       "  'maintain',\n",
       "  'data',\n",
       "  'warehouse',\n",
       "  'like',\n",
       "  'billion',\n",
       "  'row',\n",
       "  'data',\n",
       "  'center',\n",
       "  'data',\n",
       "  'driven',\n",
       "  'decision',\n",
       "  'love',\n",
       "  'like',\n",
       "  'u',\n",
       "  'problem',\n",
       "  'affect',\n",
       "  'life',\n",
       "  'real',\n",
       "  'people',\n",
       "  'user',\n",
       "  'depend',\n",
       "  'u',\n",
       "  'make',\n",
       "  'positive',\n",
       "  'change',\n",
       "  'health',\n",
       "  'life',\n",
       "  'base',\n",
       "  'scientifically',\n",
       "  'proven',\n",
       "  'peer',\n",
       "  'reviewed',\n",
       "  'methodology',\n",
       "  'designed',\n",
       "  'medical',\n",
       "  'professionalswe',\n",
       "  'respectful',\n",
       "  'diverse',\n",
       "  'dynamic',\n",
       "  'environment',\n",
       "  'engineering',\n",
       "  'first',\n",
       "  'variety',\n",
       "  'interesting',\n",
       "  'problem',\n",
       "  'affect',\n",
       "  'life',\n",
       "  'real',\n",
       "  'people',\n",
       "  'offer',\n",
       "  'budget',\n",
       "  'personal',\n",
       "  'development',\n",
       "  'expense',\n",
       "  'like',\n",
       "  'training',\n",
       "  'course',\n",
       "  'conference',\n",
       "  'book',\n",
       "  'get',\n",
       "  'three',\n",
       "  'week',\n",
       "  'paid',\n",
       "  'flexible',\n",
       "  'policy',\n",
       "  'remote',\n",
       "  'family',\n",
       "  'friendly',\n",
       "  '50',\n",
       "  'engineering',\n",
       "  'team',\n",
       "  'fully',\n",
       "  'remote',\n",
       "  'worry',\n",
       "  'result',\n",
       "  'spent',\n",
       "  'seat',\n",
       "  'delicious',\n",
       "  'nutritious',\n",
       "  'daily',\n",
       "  'lunch',\n",
       "  'snack',\n",
       "  'prepared',\n",
       "  'sam',\n",
       "  'nyc',\n",
       "  'office',\n",
       "  'site',\n",
       "  'chef',\n",
       "  'like',\n",
       "  'dealing',\n",
       "  'data',\n",
       "  'scale',\n",
       "  'processing',\n",
       "  'transforming',\n",
       "  'hundred',\n",
       "  'million',\n",
       "  'data',\n",
       "  'point',\n",
       "  'per',\n",
       "  'day',\n",
       "  'first',\n",
       "  'rate',\n",
       "  'sql',\n",
       "  'aware',\n",
       "  'limit',\n",
       "  'know',\n",
       "  'better',\n",
       "  'find',\n",
       "  'different',\n",
       "  'solution',\n",
       "  'familiar',\n",
       "  'etl',\n",
       "  'tool',\n",
       "  'problem',\n",
       "  'airflow',\n",
       "  'redshift',\n",
       "  'glue',\n",
       "  'many',\n",
       "  'system',\n",
       "  'used',\n",
       "  'alongside',\n",
       "  'data',\n",
       "  'analyst',\n",
       "  'data',\n",
       "  'scientist',\n",
       "  'help',\n",
       "  'prepare',\n",
       "  'complex',\n",
       "  'datasets',\n",
       "  'used',\n",
       "  'solve',\n",
       "  'difficult',\n",
       "  'problem'],\n",
       " ['love',\n",
       "  'number',\n",
       "  'finding',\n",
       "  'story',\n",
       "  'number',\n",
       "  'thought',\n",
       "  'tackling',\n",
       "  'complex',\n",
       "  'data',\n",
       "  'issue',\n",
       "  'make',\n",
       "  'smile',\n",
       "  'got',\n",
       "  'knack',\n",
       "  'solving',\n",
       "  'problem',\n",
       "  'want',\n",
       "  'help',\n",
       "  'drive',\n",
       "  'result',\n",
       "  'multi',\n",
       "  'million',\n",
       "  'dollar',\n",
       "  'business',\n",
       "  'answered',\n",
       "  'yes',\n",
       "  'question',\n",
       "  'lead',\n",
       "  'data',\n",
       "  'scientist',\n",
       "  'position',\n",
       "  'strategic',\n",
       "  'financial',\n",
       "  'solution',\n",
       "  'may',\n",
       "  'right',\n",
       "  'fit',\n",
       "  'strategic',\n",
       "  'looking',\n",
       "  'experienced',\n",
       "  'data',\n",
       "  'science',\n",
       "  'leader',\n",
       "  'extensive',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'join',\n",
       "  'data',\n",
       "  'science',\n",
       "  'team',\n",
       "  'already',\n",
       "  'produce',\n",
       "  'cutting',\n",
       "  'edge',\n",
       "  'model',\n",
       "  'predictive',\n",
       "  'prescriptive',\n",
       "  'analytics',\n",
       "  'person',\n",
       "  'would',\n",
       "  'responsible',\n",
       "  'leading',\n",
       "  'focused',\n",
       "  'team',\n",
       "  'conducting',\n",
       "  'data',\n",
       "  'analysis',\n",
       "  'developing',\n",
       "  'predictive',\n",
       "  'model',\n",
       "  'leveraging',\n",
       "  'data',\n",
       "  'science',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'solve',\n",
       "  'various',\n",
       "  'business',\n",
       "  'case',\n",
       "  'including',\n",
       "  'marketing',\n",
       "  'intelligence',\n",
       "  'customer',\n",
       "  'segmentation',\n",
       "  'predictive',\n",
       "  'model',\n",
       "  'operation',\n",
       "  'looking',\n",
       "  'data',\n",
       "  'scientist',\n",
       "  'support',\n",
       "  'product',\n",
       "  'sale',\n",
       "  'leadership',\n",
       "  'marketing',\n",
       "  'team',\n",
       "  'insight',\n",
       "  'gained',\n",
       "  'analyzing',\n",
       "  'external',\n",
       "  'data',\n",
       "  'ideal',\n",
       "  'candidate',\n",
       "  'adept',\n",
       "  'using',\n",
       "  'large',\n",
       "  'data',\n",
       "  'set',\n",
       "  'find',\n",
       "  'opportunity',\n",
       "  'product',\n",
       "  'process',\n",
       "  'optimization',\n",
       "  'using',\n",
       "  'model',\n",
       "  'test',\n",
       "  'effectiveness',\n",
       "  'different',\n",
       "  'course',\n",
       "  'action',\n",
       "  'must',\n",
       "  'strong',\n",
       "  'using',\n",
       "  'variety',\n",
       "  'data',\n",
       "  'mining',\n",
       "  'data',\n",
       "  'analysis',\n",
       "  'method',\n",
       "  'using',\n",
       "  'variety',\n",
       "  'data',\n",
       "  'tool',\n",
       "  'building',\n",
       "  'implementing',\n",
       "  'model',\n",
       "  'using',\n",
       "  'creating',\n",
       "  'algorithm',\n",
       "  'creating',\n",
       "  'running',\n",
       "  'simulation',\n",
       "  'must',\n",
       "  'proven',\n",
       "  'drive',\n",
       "  'business',\n",
       "  'result',\n",
       "  'data',\n",
       "  'based',\n",
       "  'insight',\n",
       "  'must',\n",
       "  'comfortable',\n",
       "  'wide',\n",
       "  'range',\n",
       "  'stakeholder',\n",
       "  'functional',\n",
       "  'team',\n",
       "  'right',\n",
       "  'candidate',\n",
       "  'passion',\n",
       "  'discovering',\n",
       "  'solution',\n",
       "  'hidden',\n",
       "  'large',\n",
       "  'data',\n",
       "  'set',\n",
       "  'stakeholder',\n",
       "  'improve',\n",
       "  'business',\n",
       "  'outcome',\n",
       "  'stakeholder',\n",
       "  'identify',\n",
       "  'opportunity',\n",
       "  'leveraging',\n",
       "  'data',\n",
       "  'drive',\n",
       "  'business',\n",
       "  'analytics',\n",
       "  'solution',\n",
       "  'research',\n",
       "  'develop',\n",
       "  'statistical',\n",
       "  'learning',\n",
       "  'model',\n",
       "  'data',\n",
       "  'analysis',\n",
       "  'communicate',\n",
       "  'result',\n",
       "  'idea',\n",
       "  'key',\n",
       "  'decision',\n",
       "  'maker',\n",
       "  'identify',\n",
       "  'valuable',\n",
       "  'data',\n",
       "  'source',\n",
       "  'automate',\n",
       "  'collection',\n",
       "  'process',\n",
       "  'undertake',\n",
       "  'preprocessing',\n",
       "  'structured',\n",
       "  'unstructured',\n",
       "  'data',\n",
       "  'analyze',\n",
       "  'large',\n",
       "  'amount',\n",
       "  'information',\n",
       "  'discover',\n",
       "  'trend',\n",
       "  'pattern',\n",
       "  'build',\n",
       "  'predictive',\n",
       "  'model',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'algorithm',\n",
       "  'present',\n",
       "  'information',\n",
       "  'using',\n",
       "  'data',\n",
       "  'visualization',\n",
       "  'technique',\n",
       "  'collaborate',\n",
       "  'sale',\n",
       "  'marketing',\n",
       "  'senior',\n",
       "  'executive',\n",
       "  'team',\n",
       "  'qualification',\n",
       "  'relevant',\n",
       "  'graduate',\n",
       "  'degree',\n",
       "  'statistic',\n",
       "  'data',\n",
       "  'science',\n",
       "  'applied',\n",
       "  'math',\n",
       "  'operation',\n",
       "  'research',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'strong',\n",
       "  'problem',\n",
       "  'solving',\n",
       "  'emphasis',\n",
       "  'sale',\n",
       "  'marketing',\n",
       "  'predictive',\n",
       "  'analytics',\n",
       "  'using',\n",
       "  'statistical',\n",
       "  'computer',\n",
       "  'language',\n",
       "  'r',\n",
       "  'python',\n",
       "  'sql',\n",
       "  'etc',\n",
       "  'manipulate',\n",
       "  'data',\n",
       "  'draw',\n",
       "  'insight',\n",
       "  'large',\n",
       "  'data',\n",
       "  'set',\n",
       "  'knowledge',\n",
       "  'variety',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'technique',\n",
       "  'clustering',\n",
       "  'decision',\n",
       "  'tree',\n",
       "  'learning',\n",
       "  'artificial',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'etc',\n",
       "  'real',\n",
       "  'world',\n",
       "  'advantage',\n",
       "  'drawback',\n",
       "  'knowledge',\n",
       "  'statistical',\n",
       "  'data',\n",
       "  'mining',\n",
       "  'technique',\n",
       "  'glm',\n",
       "  'regression',\n",
       "  'random',\n",
       "  'forest',\n",
       "  'boosting',\n",
       "  'tree',\n",
       "  'text',\n",
       "  'mining',\n",
       "  'social',\n",
       "  'network',\n",
       "  'analysis',\n",
       "  'etc',\n",
       "  'knowledge',\n",
       "  'advanced',\n",
       "  'statistical',\n",
       "  'technique',\n",
       "  'concept',\n",
       "  'regression',\n",
       "  'property',\n",
       "  'distribution',\n",
       "  'statistical',\n",
       "  'test',\n",
       "  'proper',\n",
       "  'usage',\n",
       "  'etc',\n",
       "  'application',\n",
       "  'excellent',\n",
       "  'written',\n",
       "  'verbal',\n",
       "  'communication',\n",
       "  'coordinating',\n",
       "  'across',\n",
       "  'team',\n",
       "  'drive',\n",
       "  'learn',\n",
       "  'master',\n",
       "  'new',\n",
       "  'technology',\n",
       "  'technique',\n",
       "  'using',\n",
       "  'business',\n",
       "  'intelligence',\n",
       "  'tool',\n",
       "  'e',\n",
       "  'g',\n",
       "  'powerbi',\n",
       "  'tableau',\n",
       "  'data',\n",
       "  'framework',\n",
       "  'e',\n",
       "  'g',\n",
       "  'hadoop',\n",
       "  'strategic',\n",
       "  'strategic',\n",
       "  'financial',\n",
       "  'solution',\n",
       "  'leading',\n",
       "  'consumer',\n",
       "  'finance',\n",
       "  'specializes',\n",
       "  'helping',\n",
       "  'people',\n",
       "  'much',\n",
       "  'credit',\n",
       "  'card',\n",
       "  'debt',\n",
       "  'recently',\n",
       "  'named',\n",
       "  '21st',\n",
       "  'best',\n",
       "  'new',\n",
       "  'york',\n",
       "  'best',\n",
       "  'company',\n",
       "  'certified',\n",
       "  'great',\n",
       "  'place',\n",
       "  'time',\n",
       "  'additional',\n",
       "  'honor',\n",
       "  'include',\n",
       "  'named',\n",
       "  'two',\n",
       "  'time',\n",
       "  '50',\n",
       "  'fastest',\n",
       "  'growing',\n",
       "  'company',\n",
       "  'new',\n",
       "  'york',\n",
       "  'city',\n",
       "  'prestigious',\n",
       "  'inc',\n",
       "  '500',\n",
       "  'list',\n",
       "  '500',\n",
       "  'fastest',\n",
       "  'growing',\n",
       "  'company']]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Fixing desc_tokens column'''\n",
    "#df_test = df.iloc[0:10, :]\n",
    "\n",
    "\n",
    "desc_tokens = list(df['new_description'])\n",
    "\n",
    "new_col = []\n",
    "for i, item in enumerate(desc_tokens):\n",
    "    listed = list(item.split())\n",
    "    new_col.append(listed)\n",
    "\n",
    "df['desc_tokens'] = new_col\n",
    "\n",
    "desc_tokens = list(df['desc_tokens'])\n",
    "desc_tokens[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dictionary from aggregated data descriptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from the data\n",
    "dictionary = corpora.Dictionary(desc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8880\n",
      "[[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 2), (14, 1), (15, 1), (16, 1), (17, 2), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 9), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 3), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 2), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 3), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 3), (66, 2), (67, 5), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 2), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 5), (97, 1), (98, 1), (99, 2), (100, 1), (101, 2), (102, 1), (103, 2), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 2), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 3), (122, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 3), (128, 2), (129, 2), (130, 1), (131, 1), (132, 1), (133, 1)], [(0, 1), (17, 1), (20, 1), (23, 24), (27, 2), (33, 1), (45, 1), (56, 1), (61, 1), (63, 1), (69, 2), (70, 1), (73, 1), (77, 1), (79, 1), (88, 1), (96, 2), (99, 1), (101, 1), (105, 3), (111, 2), (115, 4), (116, 1), (117, 2), (119, 1), (121, 6), (124, 2), (130, 3), (134, 1), (135, 2), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 2), (143, 1), (144, 1), (145, 4), (146, 3), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 2), (156, 1), (157, 1), (158, 1), (159, 6), (160, 2), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 3), (172, 2), (173, 1), (174, 1), (175, 1), (176, 1), (177, 2), (178, 1), (179, 1), (180, 1), (181, 1), (182, 1), (183, 1), (184, 1), (185, 1), (186, 1), (187, 1), (188, 1), (189, 1), (190, 1), (191, 4), (192, 2), (193, 1), (194, 1), (195, 1), (196, 4), (197, 1), (198, 1), (199, 1), (200, 1), (201, 1), (202, 2), (203, 1), (204, 2), (205, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (211, 2), (212, 1), (213, 1), (214, 1), (215, 1), (216, 1), (217, 2), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 2), (225, 1), (226, 1), (227, 1), (228, 1), (229, 2), (230, 3), (231, 2), (232, 1), (233, 1), (234, 1), (235, 3), (236, 1), (237, 4), (238, 1), (239, 1), (240, 1), (241, 2), (242, 1), (243, 6), (244, 2), (245, 1), (246, 4), (247, 1), (248, 1), (249, 4), (250, 1), (251, 1), (252, 1), (253, 3), (254, 7), (255, 1), (256, 1), (257, 3), (258, 2), (259, 2), (260, 1), (261, 3), (262, 2), (263, 2), (264, 2), (265, 1), (266, 1), (267, 1), (268, 1), (269, 1), (270, 1), (271, 1), (272, 1), (273, 5), (274, 1), (275, 1), (276, 1), (277, 1), (278, 2), (279, 1), (280, 2), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (286, 1), (287, 1), (288, 1), (289, 1), (290, 2), (291, 1), (292, 2), (293, 1), (294, 2), (295, 1), (296, 3), (297, 5), (298, 1), (299, 1), (300, 3), (301, 1), (302, 1), (303, 1), (304, 1), (305, 1), (306, 3), (307, 1), (308, 5), (309, 1), (310, 4), (311, 2), (312, 1), (313, 1), (314, 1), (315, 1), (316, 5), (317, 1), (318, 2), (319, 1), (320, 1), (321, 2), (322, 2), (323, 1), (324, 1), (325, 1), (326, 1), (327, 1), (328, 8), (329, 1), (330, 1), (331, 1), (332, 1), (333, 1), (334, 1), (335, 1), (336, 1), (337, 1), (338, 1), (339, 2)]]\n"
     ]
    }
   ],
   "source": [
    "# Create a bag-of-words corpus \n",
    "# also called corpus! \n",
    "\n",
    "doc_term_matrix = [dictionary.doc2bow(text) for text in desc_tokens]\n",
    "print(len(doc_term_matrix))\n",
    "print(doc_term_matrix[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(doc_term_matrix, open('doc_term_matrix.pkl', 'wb'))\n",
    "#dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of Descriptions Grouped by Bin Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Analyst\n",
      "Data Analyst\n",
      "Data Engineer\n",
      "Data Scientist\n",
      "ML/AI Researcher\n",
      "Machine Learning Engineer\n",
      "Software Engineer\n",
      "Statistical Modeler/Researcher\n",
      "Unclassified\n"
     ]
    }
   ],
   "source": [
    "by_role = df.groupby('bin_role')\n",
    "\n",
    "'''Creates a dictionary called by_role_tokens where an aggregated list of tokens is saved for each bin role\n",
    "NOTE: that desc_tokens should be a list of lists - one list of words per document'''\n",
    "\n",
    "by_role_tokens = {}\n",
    "\n",
    "for key, item in by_role:\n",
    "    aggregated_tokens = []\n",
    "    print(key)\n",
    "    #print(item['desc_tokens'])\n",
    "    desc_tokens = list(item['desc_tokens'])\n",
    "\n",
    "    #for item2 in item['desc_tokens']:\n",
    "        #aggregated_tokens = aggregated_tokens + item2\n",
    "    by_role_tokens[key] = desc_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling for each bin_role group\n",
    "\n",
    "### Question --> Can we use the same dictionary for each subject/bin role subject analysis? Or should we create new models for each role? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Analyst\n",
      "(0, '0.026*\"business\" + 0.012*\"project\" + 0.011*\"process\" + 0.010*\"system\"')\n",
      "(1, '0.028*\"business\" + 0.023*\"data\" + 0.011*\"team\" + 0.007*\"reporting\"')\n",
      "(2, '0.018*\"data\" + 0.011*\"business\" + 0.009*\"management\" + 0.009*\"system\"')\n",
      "\n",
      "Data Analyst\n",
      "(0, '0.037*\"data\" + 0.012*\"business\" + 0.008*\"team\" + 0.008*\"analysis\"')\n",
      "(1, '0.022*\"data\" + 0.006*\"analysis\" + 0.006*\"position\" + 0.006*\"preferred\"')\n",
      "(2, '0.031*\"data\" + 0.006*\"analytics\" + 0.006*\"team\" + 0.006*\"process\"')\n",
      "\n",
      "Data Engineer\n",
      "(0, '0.036*\"u\" + 0.016*\"data\" + 0.010*\"learning\" + 0.010*\"team\"')\n",
      "(1, '0.033*\"data\" + 0.008*\"cloud\" + 0.008*\"technology\" + 0.008*\"team\"')\n",
      "(2, '0.051*\"data\" + 0.009*\"business\" + 0.008*\"team\" + 0.006*\"design\"')\n",
      "\n",
      "Data Scientist\n",
      "(0, '0.025*\"data\" + 0.007*\"team\" + 0.007*\"business\" + 0.006*\"model\"')\n",
      "(1, '0.038*\"data\" + 0.010*\"science\" + 0.009*\"learning\" + 0.009*\"business\"')\n",
      "(2, '0.029*\"data\" + 0.014*\"business\" + 0.009*\"science\" + 0.008*\"learning\"')\n",
      "\n",
      "ML/AI Researcher\n",
      "(0, '0.028*\"learning\" + 0.024*\"machine\" + 0.017*\"data\" + 0.013*\"team\"')\n",
      "(1, '0.020*\"learning\" + 0.014*\"machine\" + 0.011*\"data\" + 0.011*\"system\"')\n",
      "(2, '0.013*\"data\" + 0.008*\"business\" + 0.008*\"ml\" + 0.006*\"solution\"')\n",
      "\n",
      "Machine Learning Engineer\n",
      "(0, '0.015*\"data\" + 0.007*\"machine\" + 0.007*\"learning\" + 0.006*\"interview\"')\n",
      "(1, '0.009*\"computer\" + 0.007*\"team\" + 0.006*\"technology\" + 0.006*\"innovation\"')\n",
      "(2, '0.012*\"data\" + 0.007*\"learning\" + 0.006*\"team\" + 0.006*\"technology\"')\n",
      "\n",
      "Software Engineer\n",
      "(0, '0.011*\"team\" + 0.007*\"software\" + 0.007*\"learning\" + 0.007*\"engineer\"')\n",
      "(1, '0.017*\"software\" + 0.011*\"development\" + 0.010*\"team\" + 0.008*\"system\"')\n",
      "(2, '0.011*\"team\" + 0.010*\"software\" + 0.009*\"product\" + 0.007*\"technology\"')\n",
      "\n",
      "Statistical Modeler/Researcher\n",
      "(0, '0.011*\"model\" + 0.009*\"business\" + 0.009*\"capital\" + 0.006*\"modeling\"')\n",
      "(1, '0.005*\"model\" + 0.004*\"data\" + 0.003*\"modeling\" + 0.003*\"relevant\"')\n",
      "(2, '0.016*\"data\" + 0.008*\"model\" + 0.008*\"statistical\" + 0.007*\"risk\"')\n",
      "\n",
      "Unclassified\n",
      "(0, '0.019*\"research\" + 0.007*\"position\" + 0.006*\"program\" + 0.006*\"data\"')\n",
      "(1, '0.013*\"team\" + 0.010*\"data\" + 0.008*\"product\" + 0.007*\"ã\"')\n",
      "(2, '0.009*\"management\" + 0.008*\"business\" + 0.008*\"risk\" + 0.007*\"project\"')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 3 subjects/topics for each bin_role grouping\n",
    "\n",
    "NUM_TOPICS = 3\n",
    "\n",
    "for key, values in by_role_tokens.items():\n",
    "    \n",
    "    doc_term_matrix = [dictionary.doc2bow(text) for text in values]\n",
    "    \n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "\n",
    "    topics = ldamodel.print_topics(num_words=4)\n",
    "    print(key)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same as above but with new dictionaries for each bin role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Analyst\n",
      "(0, '0.025*\"data\" + 0.019*\"business\" + 0.010*\"team\" + 0.006*\"system\"')\n",
      "(1, '0.033*\"business\" + 0.016*\"data\" + 0.011*\"process\" + 0.011*\"project\"')\n",
      "(2, '0.019*\"business\" + 0.008*\"analysis\" + 0.008*\"project\" + 0.007*\"system\"')\n",
      "\n",
      "Data Analyst\n",
      "(0, '0.034*\"data\" + 0.013*\"business\" + 0.009*\"analysis\" + 0.008*\"team\"')\n",
      "(1, '0.043*\"data\" + 0.013*\"business\" + 0.009*\"team\" + 0.007*\"analytics\"')\n",
      "(2, '0.031*\"data\" + 0.007*\"information\" + 0.006*\"management\" + 0.006*\"support\"')\n",
      "\n",
      "Data Engineer\n",
      "(0, '0.044*\"u\" + 0.021*\"data\" + 0.014*\"learning\" + 0.011*\"team\"')\n",
      "(1, '0.047*\"data\" + 0.008*\"team\" + 0.008*\"business\" + 0.008*\"system\"')\n",
      "(2, '0.043*\"data\" + 0.009*\"technology\" + 0.009*\"team\" + 0.009*\"cloud\"')\n",
      "\n",
      "Data Scientist\n",
      "(0, '0.037*\"data\" + 0.011*\"learning\" + 0.010*\"science\" + 0.009*\"team\"')\n",
      "(1, '0.037*\"data\" + 0.012*\"business\" + 0.009*\"science\" + 0.008*\"team\"')\n",
      "(2, '0.027*\"data\" + 0.009*\"business\" + 0.008*\"team\" + 0.007*\"learning\"')\n",
      "\n",
      "ML/AI Researcher\n",
      "(0, '0.034*\"learning\" + 0.027*\"machine\" + 0.012*\"team\" + 0.009*\"deep\"')\n",
      "(1, '0.018*\"data\" + 0.016*\"learning\" + 0.013*\"machine\" + 0.008*\"solution\"')\n",
      "(2, '0.021*\"data\" + 0.015*\"learning\" + 0.014*\"team\" + 0.012*\"machine\"')\n",
      "\n",
      "Machine Learning Engineer\n",
      "(0, '0.022*\"data\" + 0.017*\"learning\" + 0.013*\"engineering\" + 0.013*\"computer\"')\n",
      "(1, '0.031*\"data\" + 0.013*\"technology\" + 0.009*\"develop\" + 0.009*\"team\"')\n",
      "(2, '0.012*\"computer\" + 0.012*\"data\" + 0.011*\"team\" + 0.011*\"learning\"')\n",
      "\n",
      "Software Engineer\n",
      "(0, '0.018*\"software\" + 0.011*\"development\" + 0.010*\"team\" + 0.010*\"system\"')\n",
      "(1, '0.012*\"team\" + 0.012*\"software\" + 0.008*\"development\" + 0.007*\"technology\"')\n",
      "(2, '0.014*\"team\" + 0.014*\"software\" + 0.009*\"system\" + 0.009*\"data\"')\n",
      "\n",
      "Statistical Modeler/Researcher\n",
      "(0, '0.039*\"data\" + 0.014*\"science\" + 0.012*\"training\" + 0.010*\"analytics\"')\n",
      "(1, '0.019*\"model\" + 0.018*\"data\" + 0.013*\"risk\" + 0.011*\"modeling\"')\n",
      "(2, '0.021*\"business\" + 0.021*\"model\" + 0.014*\"statistical\" + 0.013*\"data\"')\n",
      "\n",
      "Unclassified\n",
      "(0, '0.008*\"management\" + 0.007*\"project\" + 0.007*\"risk\" + 0.007*\"business\"')\n",
      "(1, '0.013*\"team\" + 0.008*\"product\" + 0.008*\"data\" + 0.008*\"technology\"')\n",
      "(2, '0.018*\"research\" + 0.012*\"data\" + 0.010*\"ã\" + 0.006*\"analysis\"')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 3 subjects/topics for each bin_role grouping\n",
    "\n",
    "NUM_TOPICS = 3\n",
    "\n",
    "for key, values in by_role_tokens.items():\n",
    "    \n",
    "    # Create a dictionary from the data\n",
    "    dictionary = corpora.Dictionary(values)\n",
    "    \n",
    "    doc_term_matrix = [dictionary.doc2bow(text) for text in values]\n",
    "    \n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "\n",
    "    topics = ldamodel.print_topics(num_words=4)\n",
    "    print(key)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyLDAvis\n",
    "\n",
    "Establishes most salient terms and creates distance plot maps of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "#lda = gensim.models.ldamodel.LdaModel.load('model5.gensim')\n",
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Model\n",
    "### Using Multinomial Naive Bayes Classification from scikit-learn\n",
    "\n",
    "As seen on: \n",
    "https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk\n",
    "\n",
    "The following model uses the one-hot-encoded description column in the data frame as the set of X features and evaluates the bin role as the response (y variable). \n",
    "\n",
    "## 1. Using TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_descriptions(by_role):\n",
    "    '''Creates concatenated/joined text for job descriptions for each bin role'''\n",
    "    roles = []\n",
    "    merged_desc = []\n",
    "    for key, items in by_role.indices.items():\n",
    "        # concatenate strings in descriptions column (by search_role grouping) and append to list of merged descriptions\n",
    "        string=(\" \").join(description for description in df.loc[items,'new_description'])\n",
    "        merged_desc.append(string)\n",
    "        roles.append(key)\n",
    "\n",
    "    return merged_desc, roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''descriptions variable is list of concatenated descriptions separated by bin role'''\n",
    "descriptions, roles = group_descriptions(by_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.5596846846846847\n"
     ]
    }
   ],
   "source": [
    "# generate matrix of word vectors \n",
    "\n",
    "tf=TfidfVectorizer()\n",
    "text_tf= tf.fit_transform(df['new_description'])\n",
    "\n",
    "# Build the training and sets using the one-hot-encoded TF-IDF vectorizer\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_tf, df['bin_role'], test_size=0.10, random_state=1)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Bag of Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.8209459459459459\n"
     ]
    }
   ],
   "source": [
    "# You can generate document term matrix by using scikit-learn's CountVectorizer.\n",
    "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts= cv.fit_transform(df['new_description'])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_counts, df['bin_role'], test_size=0.3, random_state=1)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.8543543543543544\n"
     ]
    }
   ],
   "source": [
    "# for trigram model - change ngram_range\n",
    "\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (3,3),tokenizer = token.tokenize)\n",
    "text_counts= cv.fit_transform(df['new_description'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_counts, df['bin_role'], test_size=0.3, random_state=1)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Classification Model\n",
    "Reference: https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf # trigram model from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin_role                                               Data Engineer\n",
      "company                                                    Noom Inc.\n",
      "description         At Noom, we use scientifically proven methods...\n",
      "location                                                New York, NY\n",
      "title                                                 Data Engineer \n",
      "url                https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...\n",
      "desc_tokens        [noom, scientifically, proven, method, help, u...\n",
      "new_description    noom scientifically proven method help user cr...\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' At Noom, we use scientifically proven methods to help our users create healthier lifestyles, and manage important conditions like Type-II Diabetes, Obesity, and Hypertension. Our Engineering team is at the forefront of this challenge, solving complex technical problems that center around habits, behavior, and lifestyle. We are looking for a Data Engineer to join our Data team and help us improve and maintain our Data Warehouse. If you like to work with billions of rows of data and be at the center of data-driven decisions, we’ll love working with you. What You’ll Like About Us We work on problems that affect the lives of real people. Our users depend on us to make positive changes to their health and their lives.We base our work on scientifically-proven, peer-reviewed methodologies that are designed by medical professionalsWe’re a respectful, diverse, and dynamic environment in which Engineering is a first-class citizen, and where you’ll be able to work on a variety of interesting problems that affect the lives of real people.We offer a generous budget for personal development expenses like training courses, conferences, and books.You’ll get three weeks’ paid vacation and a flexible work policy that is remote- and family-friendly (about 50% of our engineering team is fully remote). We worry about results, not time spent in seats.Delicious (and nutritious) daily lunches and snacks prepared by Sam, our NYC office on-site chef. What We’ll Like About You You have experience dealing with data at scale, processing and transforming hundreds of millions of data points per day.You have first-rate SQL skills, but you are aware of its limits. You know when to use it, and when it’s better to find a different solution.You are familiar with ETL tools and problems. We use Airflow, Redshift, Glue, and many other systems.You’re used to work alongside data analysts and data scientists to help them prepare complex datasets that can be used to solve difficult problems. Job Type: Full-time'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.iloc[0,:])\n",
    "\n",
    "df.loc[0, 'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Role Label:  ['Data Engineer']\n"
     ]
    }
   ],
   "source": [
    "test_description = 'At Noom, we use scientifically proven methods to help our users create healthier lifestyles, and manage important conditions like Type-II Diabetes, Obesity, and Hypertension. Our Engineering team is at the forefront of this challenge, solving complex technical problems that center around habits, behavior, and lifestyle. We are looking for a Data Engineer to join our Data team and help us improve and maintain our Data Warehouse. If you like to work with billions of rows of data and be at the center of data-driven decisions, we’ll love working with you. What You’ll Like About Us We work on problems that affect the lives of real people. Our users depend on us to make positive changes to their health and their lives.We base our work on scientifically-proven, peer-reviewed methodologies that are designed by medical professionalsWe’re a respectful, diverse, and dynamic environment in which Engineering is a first-class citizen, and where you’ll be able to work on a variety of interesting problems that affect the lives of real people.We offer a generous budget for personal development expenses like training courses, conferences, and books.You’ll get three weeks’ paid vacation and a flexible work policy that is remote- and family-friendly (about 50% of our engineering team is fully remote). We worry about results, not time spent in seats.Delicious (and nutritious) daily lunches and snacks prepared by Sam, our NYC office on-site chef. What We’ll Like About You You have experience dealing with data at scale, processing and transforming hundreds of millions of data points per day.You have first-rate SQL skills, but you are aware of its limits. You know when to use it, and when it’s better to find a different solution.You are familiar with ETL tools and problems. We use Airflow, Redshift, Glue, and many other systems.You’re used to work alongside data analysts and data scientists to help them prepare complex datasets that can be used to solve difficult problems. Job Type: Full-time'\n",
    "\n",
    "print('Predicted Role Label: ', clf.predict(cv.transform([test_description])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin_role                                              Data Scientist\n",
      "company                                Strategic Financial Solutions\n",
      "description         Overview\\nDo you love numbers and finding the...\n",
      "location                                          New York, NY 10018\n",
      "title                                           Lead Data Scientist \n",
      "url                https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...\n",
      "desc_tokens        [love, number, finding, story, number, thought...\n",
      "new_description    love number finding story number thought tackl...\n",
      "Name: 1, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Overview\\nDo you love numbers and finding the story in the numbers? Does the thought of tackling a complex data issue make you smile? Have you got a knack for solving problems? Do you want to help drive the results of a multi-million dollar business? If you have answered “yes” to these questions, the Lead Data Scientist position at Strategic Financial Solutions may be the right fit for you. Strategic is looking for an experienced Data Science leader with extensive machine learning experience to join our Data Science Team, which already produces cutting-edge models for predictive and prescriptive analytics. The person in this role would be responsible for leading a focused team conducting data analysis and developing predictive models leveraging data science and machine learning to solve various business use cases, including marketing intelligence, customer segmentation, and predictive models for operations. We are looking for a Data Scientist who will support our product, sales, leadership and marketing teams with insights gained from analyzing company and external data. The ideal candidate is adept at using large data sets to find opportunities for product and process optimization and using models to test the effectiveness of different courses of action. They must have strong experience using a variety of data mining/data analysis methods, using a variety of data tools, building and implementing models, using/creating algorithms and creating/running simulations. They must have a proven ability to drive business results with their data-based insights. They must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes. Responsibilities\\nWork with stakeholders to identify opportunities for leveraging company data to drive business analytics solutions.\\nResearch and develop statistical learning models for data analysis\\nCommunicate results and ideas to key decision makers\\nIdentify valuable data sources and automate collection processes\\nUndertake preprocessing of structured and unstructured data\\nAnalyze large amounts of information to discover trends and patterns\\nBuild predictive models and machine-learning algorithms\\nPresent information using data visualization techniques\\nCollaborate with sales, marketing and senior executive teams\\nQualifications\\n~5-8 years of relevant experience\\nGraduate degree in Statistics, Data Science, Applied Math, Operations Research, or Computer Science.\\nStrong problem-solving skills with an emphasis on sales and marketing predictive analytics.\\nExperience using statistical computer languages (R, Python, SQL, etc.) to manipulate data and draw insights from large data sets.\\nKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks.\\nKnowledge and experience in statistical and data mining techniques: GLM/Regression, Random Forest, Boosting, Trees, text mining, social network analysis, etc.\\nKnowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications.\\nExcellent written and verbal communication skills for coordinating across teams.\\nA drive to learn and master new technologies and techniques.\\nExperience using business intelligence tools (e.g. PowerBI, Tableau) and data frameworks (e.g. Hadoop)\\nAbout Strategic:\\nStrategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States.'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.iloc[1,:])\n",
    "df.loc[1, 'description']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Role Label:  ['Data Scientist']\n"
     ]
    }
   ],
   "source": [
    "test_description = 'creating algorithms and creating/running simulations. They must have a proven ability to drive business results with their data-based insights. They must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes. Responsibilities\\nWork with stakeholders to identify opportunities for leveraging company data to drive business analytics solutions.\\nResearch and develop statistical learning models for data analysis\\nCommunicate results and ideas to key decision makers\\nIdentify valuable data sources and automate collection processes\\nUndertake preprocessing of structured and unstructured data\\nAnalyze large amounts of information to discover trends and patterns\\nBuild predictive models and machine-learning algorithms\\nPresent information using data visualization techniques\\nCollaborate with sales, marketing and senior executive teams\\nQualifications\\n~5-8 years of relevant experience\\nGraduate degree in Statistics, Data Science, Applied Math, Operations Research, or Computer Science.\\nStrong problem-solving skills with an emphasis on sales and marketing predictive analytics.\\nExperience using statistical computer languages (R, Python, SQL, etc.) to manipulate data and draw insights from large data sets.\\nKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks.\\nKnowledge and experience in statistical and data mining techniques: GLM/Regression, Random Forest, Boosting, Trees, text mining, social network analysis, etc.\\nKnowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications.\\nExcellent written and verbal communication skills for coordinating across teams.\\nA drive to learn and master new technologies and techniques.\\nExperience using business intelligence tools (e.g. PowerBI, Tableau) and data frameworks (e.g. Hadoop)\\nAbout Strategic:\\nStrategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States.'\n",
    "\n",
    "print('Predicted Role Label: ', clf.predict(cv.transform([test_description])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Similarity Between Aggregated Posts Grouped by Search Role\n",
    "- Uses Cosine Similarity score\n",
    "\n",
    "As seen on:\n",
    "https://www.datacamp.com/courses/feature-engineering-for-nlp-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 3476488 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-221-e9025548d70e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             raise ValueError(\n\u001b[0;32m--> 392\u001b[0;31m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             )\n\u001b[1;32m    394\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 3476488 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "# load model and create Doc object\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "for i, merged_text in enumerate(descriptions):\n",
    "    doc = nlp(merged_text)\n",
    "\n",
    "\n",
    "    for token1 in doc:\n",
    "        for token2 in doc:\n",
    "            print(token1.text, token2.text, token1.similarity(token2))\n",
    "            print()\n",
    "        #print(token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
