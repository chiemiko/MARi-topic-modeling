{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-49c45ac8e322>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#import spacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import glob\n",
    "from os import path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data_processed/concatenated_data_cleaned_labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'English' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9cafc8da1e1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create English parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#spacy.load('en')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'English' is not defined"
     ]
    }
   ],
   "source": [
    "parser = English()\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('\\n'):\n",
    "            continue\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "        print(lemma)\n",
    "    \n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "# List of stopwords to upload\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "desc_tokens = []\n",
    "\n",
    "for index, row in df.iterrows():    \n",
    "    tokens = tokenize(row['description'])\n",
    "    #print(index)\n",
    "    # Filters tokens to only words of length 4 or more\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    # Filters tokens that are in list of stopwords\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    # Lemmatizes tokens to their roots\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    desc_tokens.append(tokens)\n",
    "\n",
    "df['desc_tokens'] = desc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin_role</th>\n",
       "      <th>company</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>desc_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Noom Inc.</td>\n",
       "      <td>At Noom, we use scientifically proven methods...</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>[scientifically, prove, method, user, create, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Strategic Financial Solutions</td>\n",
       "      <td>Overview\\nDo you love numbers and finding the...</td>\n",
       "      <td>New York, NY 10018</td>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>[overview, numbers, finding, story, numbers, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Averity</td>\n",
       "      <td>We are one of the world's premiere travel tec...</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Manager, Data Science</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>[world, premiere, travel, technology, company,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Lenmar Consulting</td>\n",
       "      <td>Medical Data Scientist Must have at least 2 y...</td>\n",
       "      <td>Woodcliff Lake, NJ</td>\n",
       "      <td>Medical Data Scientist</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>[medical, scientist, least, years, clinical, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Memorial Sloan Kettering</td>\n",
       "      <td>&lt;h2 class=\"jobSectionHeader\"&gt;Company Overview...</td>\n",
       "      <td>New York, NY 10017</td>\n",
       "      <td>Sr Strategic Consultant / Data Scientist (Pati...</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>[class=\"jobsectionheader\"&gt;company, overview, m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bin_role                        company  \\\n",
       "0   Data Engineer                      Noom Inc.   \n",
       "1  Data Scientist  Strategic Financial Solutions   \n",
       "2  Data Scientist                        Averity   \n",
       "3  Data Scientist              Lenmar Consulting   \n",
       "4  Data Scientist       Memorial Sloan Kettering   \n",
       "\n",
       "                                         description            location  \\\n",
       "0   At Noom, we use scientifically proven methods...        New York, NY   \n",
       "1   Overview\\nDo you love numbers and finding the...  New York, NY 10018   \n",
       "2   We are one of the world's premiere travel tec...        New York, NY   \n",
       "3   Medical Data Scientist Must have at least 2 y...  Woodcliff Lake, NJ   \n",
       "4   <h2 class=\"jobSectionHeader\">Company Overview...  New York, NY 10017   \n",
       "\n",
       "                                               title  \\\n",
       "0                                     Data Engineer    \n",
       "1                               Lead Data Scientist    \n",
       "2                             Manager, Data Science    \n",
       "3                            Medical Data Scientist    \n",
       "4  Sr Strategic Consultant / Data Scientist (Pati...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "1  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "2  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "3  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "4  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "\n",
       "                                         desc_tokens  \n",
       "0  [scientifically, prove, method, user, create, ...  \n",
       "1  [overview, numbers, finding, story, numbers, t...  \n",
       "2  [world, premiere, travel, technology, company,...  \n",
       "3  [medical, scientist, least, years, clinical, r...  \n",
       "4  [class=\"jobsectionheader\">company, overview, m...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data_processed/concatenated_data_cleaned_labeled_preprocessed.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
